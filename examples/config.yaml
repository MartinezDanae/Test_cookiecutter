# general
batch_size: 16

optimizer:
  algo: 'SGD' # adagrad, AdamW, sgd, adam
  lr: 0.001
  momentum: 0.0001

loss: cross_entropy
max_epoch: 1
exp_name: my_exp_sgd_cnn
num_workers: 0
# set to null to avoid setting a seed (can speed up GPU computation, but
# results will not be reproducible)
seed: 1234


# loggers
experiment_loggers:
  tensorboard: null  # no parameters for tensorboard
  aim:
    # change this to an absolute path to always use the same aim db file
    log_folder: ./
  # comet: null  << uncomment to use comet - see README for more info on setup 

# architecture
hidden_dim: 256
num_classes: 10
architecture: simple_cnn #simple_mlp # simple_cnn

# CNN-specific
in_channels: 1     # 1 for MNIST, 3 for RGB datasets
hidden_channels: 32
kernel_size: 3
input_hw: [28, 28] # used to compute the flatten size

# datamodule 
#dataclass: FashionMnistTVDM # remove if you want to use original dataclass for load data
normalize: [0.1307, 0.3081] # per-channel mean/std for grayscale FashionMNIST

# here wew centralize the metric and the mode to use in both early stopping and
# best checkpoint selection. If instead you want to use different metric/mode,
# remove this section and define them directly in the early_stopping / best_checkpoint blocks.
metric_to_use: 'val_loss'
mode_to_use: 'min'

# early stopping
early_stopping:
  metric: ${metric_to_use}
  mode: ${mode_to_use}
  patience: 3

# best checkpoint params
best_checkpoint:
  metric: ${metric_to_use}
  mode: ${mode_to_use}
  every_n_epochs: 1